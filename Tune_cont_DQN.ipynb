{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Continuous DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 12:29:22.175943 139735645222720 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "# import environment. set directory to find it.\n",
    "path='/home/lorenzo/Desktop/FirmsPricing_ContObs'\n",
    "os.chdir(path)\n",
    "from MA_Firms_Pricing_ContObs import MultiAgentFirmsPricingContinuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment config and other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment with the given configs\n",
    "ENV_CONFIG = {\"num_agents\": 2,\n",
    "              \"max_steps\":  10**9,\n",
    "              \"p_min\":1.2,\n",
    "              \"p_max\":2,}\n",
    "env=MultiAgentFirmsPricingContinuous(env_config=ENV_CONFIG)\n",
    "\n",
    "# Define policies\n",
    "def gen_policy():\n",
    "    return(None, env.observation_space, env.action_space, {})\n",
    "\n",
    "policy_graphs = dict() \n",
    "for i in range(env.num):\n",
    "    policy_graphs['agent_'+str(i)]=gen_policy()\n",
    "\n",
    "# Function for mapping agents to policies\n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks for custom metrics\n",
    "def on_episode_start(info):\n",
    "    episode = info[\"episode\"]\n",
    "    episode.user_data[\"delta0\"] = []\n",
    "    episode.user_data[\"delta1\"] = []\n",
    "    episode.user_data[\"price0\"] = []\n",
    "    episode.user_data[\"price1\"] = []\n",
    "\n",
    "def on_episode_step(info):\n",
    "    episode = info[\"episode\"]\n",
    "    delta0 = (episode.prev_reward_for(agent_id='agent_0') - 0.22589)/(0.337472 - 0.22589)\n",
    "    delta1 = (episode.prev_reward_for(agent_id='agent_1') - 0.22589)/(0.337472 - 0.22589)\n",
    "    price0 = episode.last_raw_obs_for(agent_id='agent_0')[0]\n",
    "    price1 = episode.last_raw_obs_for(agent_id='agent_0')[1]\n",
    "    episode.user_data[\"delta0\"].append(delta0)\n",
    "    episode.user_data[\"delta1\"].append(delta1)\n",
    "    episode.user_data[\"price0\"].append(price0)\n",
    "    episode.user_data[\"price1\"].append(price1)\n",
    "\n",
    "def on_episode_end(info):\n",
    "    episode = info[\"episode\"]\n",
    "    delta0 = np.mean(episode.user_data[\"delta0\"])\n",
    "    delta1 = np.mean(episode.user_data[\"delta1\"])\n",
    "    price0 = np.mean(episode.user_data[\"price0\"])\n",
    "    price1 = np.mean(episode.user_data[\"price1\"])\n",
    "    episode.custom_metrics[\"delta0\"] = delta0\n",
    "    episode.custom_metrics[\"delta1\"] = delta1\n",
    "    episode.custom_metrics[\"price0\"] = price0\n",
    "    episode.custom_metrics[\"price1\"] = price1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-03 12:31:02,765\tWARNING worker.py:1349 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-08-03 12:31:02,766\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-03_12-31-02_766596_4828/logs.\n",
      "2019-08-03 12:31:02,881\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:52638 to respond...\n",
      "2019-08-03 12:31:02,999\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:20462 to respond...\n",
      "2019-08-03 12:31:03,001\tINFO services.py:809 -- Starting Redis shard with 2.05 GB max memory.\n",
      "2019-08-03 12:31:03,017\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-03_12-31-02_766596_4828/logs.\n",
      "2019-08-03 12:31:03,019\tINFO services.py:1475 -- Starting the Plasma object store with 3.07 GB memory using /dev/shm.\n",
      "2019-08-03 12:31:03,117\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2019-08-03 12:31:03,125\tWARNING trainer.py:685 -- The `policy_graphs` config has been renamed to `policies`.\n",
      "2019-08-03 12:31:03,139\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "W0803 12:31:03.175817 139735645222720 deprecation_wrapper.py:119] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/tune/logger.py:134: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.\n",
      "\n",
      "W0803 12:31:03.178020 139735645222720 deprecation_wrapper.py:119] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/tune/logger.py:139: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "2019-08-03 12:31:03,200\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2019-08-03 12:31:03,254\tWARNING worker.py:352 -- WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "2019-08-03 12:31:03,265\tWARNING worker.py:352 -- WARNING: Falling back to serializing objects of type <class 'mtrand.RandomState'> by using pickle. This may be inefficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 2.3/10.2 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-03 12:31:03,329\tWARNING util.py:145 -- The `start_trial` operation took 0.1964702606201172 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 2.3/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:05.163333 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:05,461\tINFO rollout_worker.py:310 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:05.462272: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:05.466744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193995000 Hz\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:05.467215: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557e86c96c80 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:05.467230: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:05.482605 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/distributional_q_model.py:80: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Use keras.layers.dense instead.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:05.484624 139903247939392 deprecation.py:506] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:06.055961 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:06.064538 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:123: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:06.065228 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:125: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:06.109497: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:06,117\tINFO dynamic_tf_policy.py:323 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m { 'actions': <tf.Tensor 'agent_0/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'dones': <tf.Tensor 'agent_0/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'new_obs': <tf.Tensor 'agent_0/new_obs:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'obs': <tf.Tensor 'agent_0/observation:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'q_values': <tf.Tensor 'agent_0/q_values:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'rewards': <tf.Tensor 'agent_0/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'weights': <tf.Tensor 'agent_0/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:06.278263 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:63: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:06.290807 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:08,128\tINFO rollout_worker.py:733 -- Built policy map: {'agent_0': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f3d789767f0>, 'agent_1': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f3cb033c748>}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:08,128\tINFO rollout_worker.py:734 -- Built preprocessor map: {'agent_0': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3d78976748>, 'agent_1': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3cb03a5e80>}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:08,129\tINFO rollout_worker.py:347 -- Built filter map: {'agent_0': MeanStdFilter((2,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0)), 'agent_1': MeanStdFilter((2,), True, True, None, (n=0, mean_mean=0.0, mean_std=0.0), (n=0, mean_mean=0.0, mean_std=0.0))}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:08,136\tINFO actors.py:108 -- Trying to create 4 colocated actors\n",
      "\u001b[2m\u001b[36m(pid=4912)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4912)\u001b[0m W0803 12:31:09.230124 140152693409600 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4912)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4912)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4911)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4911)\u001b[0m W0803 12:31:09.224688 140008121956160 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4911)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4911)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4910)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4910)\u001b[0m W0803 12:31:09.247816 139737747187520 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4910)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4910)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4908)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4908)\u001b[0m W0803 12:31:09.258690 140220403017536 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4908)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4908)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:09,612\tINFO actors.py:101 -- Got 4 colocated actors of 4\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:10.682329 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m W0803 12:31:10.750744 139903247939392 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11,013\tINFO rollout_worker.py:310 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11.031431: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11.036388: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193995000 Hz\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11.036614: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557eaaabe6e0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11.036627: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:11.055319 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/distributional_q_model.py:80: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Use keras.layers.dense instead.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:11.058066 139957254342464 deprecation.py:506] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m WARNING: Logging before flag parsing goes to stderr.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:11.590717 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:11.721590 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:11.734001 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:123: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:11.734756 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:125: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11.788688: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:11,797\tINFO dynamic_tf_policy.py:323 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m { 'actions': <tf.Tensor 'agent_0/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'dones': <tf.Tensor 'agent_0/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'new_obs': <tf.Tensor 'agent_0/new_obs:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'obs': <tf.Tensor 'agent_0/observation:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'q_values': <tf.Tensor 'agent_0/q_values:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'rewards': <tf.Tensor 'agent_0/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'weights': <tf.Tensor 'agent_0/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:11,935\tINFO rollout_worker.py:310 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:11.953283: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:11.960012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3193995000 Hz\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:11.960438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c45b58ba70 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:11.960469: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:11.976518 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/distributional_q_model.py:80: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Use keras.layers.dense instead.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:11.980295 139767708759872 deprecation.py:506] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:12.009026 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:63: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:12.026565 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:12.632648 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:12.642719 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:123: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:12.643344 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:125: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m 2019-08-03 12:31:12.699138: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:12.913450 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:63: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:12.928553 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,488\tINFO rollout_worker.py:442 -- Generating sample batch of size 128\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,490\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   1: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   2: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   3: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   4: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   5: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   6: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   7: { 'agent_0': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m        'agent_1': np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)}}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,491\tINFO sampler.py:305 -- Info return from env: { 0: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   1: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   2: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   3: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   4: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   5: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   6: {'agent_0': {}, 'agent_1': {}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   7: {'agent_0': {}, 'agent_1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,491\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((2,), dtype=float64, min=1.75, max=1.75, mean=1.75)\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,491\tINFO sampler.py:407 -- Filtered obs: np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0)\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,496\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m { 'agent_0': [ { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_0',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'}],\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'agent_1': [ { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'data': { 'agent_id': 'agent_1',\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'info': {},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'obs': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                            'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                  'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,497\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,591\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m { 'agent_0': ( np.ndarray((8,), dtype=int64, min=0.0, max=3.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                [],\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'q_values': np.ndarray((8, 5), dtype=float32, min=0.0, max=0.0, mean=0.0)}),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'agent_1': ( np.ndarray((8,), dtype=int64, min=0.0, max=4.0, mean=2.625),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                [],\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                { 'q_values': np.ndarray((8, 5), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,822\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m { 'agent_0': { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=4.0, mean=1.875),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'eps_id': np.ndarray((16,), dtype=int64, min=1788956441.0, max=1788956441.0, mean=1788956441.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'infos': np.ndarray((16,), dtype=object, head={'delta': 0.9823582278189381}),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'new_obs': np.ndarray((16, 2), dtype=float32, min=-2.366, max=2.872, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'obs': np.ndarray((16, 2), dtype=float32, min=-2.667, max=2.872, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=4.0, mean=1.812),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'prev_rewards': np.ndarray((16,), dtype=float32, min=0.0, max=0.336, mean=0.227),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'q_values': np.ndarray((16, 5), dtype=float32, min=-0.631, max=1.278, mean=0.468),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'rewards': np.ndarray((16,), dtype=float32, min=0.231, max=0.912, mean=0.648),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          't': np.ndarray((16,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'weights': np.ndarray((16,), dtype=float32, min=2.303, max=2.48, mean=2.397)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'agent_1': { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=4.0, mean=1.688),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'agent_index': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'eps_id': np.ndarray((16,), dtype=int64, min=1788956441.0, max=1788956441.0, mean=1788956441.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'infos': np.ndarray((16,), dtype=object, head={'delta': 0.64919621419251}),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'new_obs': np.ndarray((16, 2), dtype=float32, min=-2.366, max=2.872, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'obs': np.ndarray((16, 2), dtype=float32, min=-2.667, max=2.872, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=4.0, mean=1.688),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'prev_rewards': np.ndarray((16,), dtype=float32, min=0.0, max=0.407, mean=0.343),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'q_values': np.ndarray((16, 5), dtype=float32, min=-1.261, max=2.972, mean=0.843),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'rewards': np.ndarray((16,), dtype=float32, min=0.343, max=1.184, mean=1.01),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          't': np.ndarray((16,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                          'weights': np.ndarray((16,), dtype=float32, min=2.303, max=2.841, mean=2.562)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m 2019-08-03 12:31:14,852\tINFO rollout_worker.py:476 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'policy_batches': { 'agent_0': { 'data': { 'actions': np.ndarray((128,), dtype=int64, min=0.0, max=4.0, mean=1.844),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'agent_index': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'eps_id': np.ndarray((128,), dtype=int64, min=221553930.0, max=1788956441.0, mean=864181456.125),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'infos': np.ndarray((128,), dtype=object, head={'delta': 0.9823582278189381}),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'new_obs': np.ndarray((128, 2), dtype=float32, min=-3.225, max=2.872, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'obs': np.ndarray((128, 2), dtype=float32, min=-3.225, max=2.872, mean=-0.063),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'prev_actions': np.ndarray((128,), dtype=int64, min=0.0, max=4.0, mean=1.688),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'prev_rewards': np.ndarray((128,), dtype=float32, min=0.0, max=0.437, mean=0.314),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'q_values': np.ndarray((128, 5), dtype=float32, min=-1.442, max=2.123, mean=0.27),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'rewards': np.ndarray((128,), dtype=float32, min=0.231, max=1.248, mean=0.924),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              't': np.ndarray((128,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'unroll_id': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'weights': np.ndarray((128,), dtype=float32, min=2.283, max=2.546, mean=2.37)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                    'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                       'agent_1': { 'data': { 'actions': np.ndarray((128,), dtype=int64, min=0.0, max=4.0, mean=2.148),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'agent_index': np.ndarray((128,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'eps_id': np.ndarray((128,), dtype=int64, min=221553930.0, max=1788956441.0, mean=864181456.125),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'infos': np.ndarray((128,), dtype=object, head={'delta': 0.64919621419251}),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'new_obs': np.ndarray((128, 2), dtype=float32, min=-3.225, max=2.872, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'obs': np.ndarray((128, 2), dtype=float32, min=-3.225, max=2.872, mean=-0.063),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'prev_actions': np.ndarray((128,), dtype=int64, min=0.0, max=4.0, mean=2.062),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'prev_rewards': np.ndarray((128,), dtype=float32, min=0.0, max=0.407, mean=0.257),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'q_values': np.ndarray((128, 5), dtype=float32, min=-1.898, max=2.972, mean=-0.041),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'rewards': np.ndarray((128,), dtype=float32, min=0.127, max=1.184, mean=0.744),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              't': np.ndarray((128,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'unroll_id': np.ndarray((128,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                              'weights': np.ndarray((128,), dtype=float32, min=2.259, max=2.841, mean=2.438)},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m                                    'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m W0803 12:31:15.013918 139957254342464 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=4907)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m W0803 12:31:16.025788 139767708759872 deprecation.py:323] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=5033)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO rollout_worker.py:566 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m { 'count': 64,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'policy_batches': { 'agent_0': { 'data': { 'actions': np.ndarray((64,), dtype=int64, min=0.0, max=4.0, mean=2.125),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'batch_indexes': np.ndarray((64,), dtype=int64, min=9.0, max=5073.0, mean=2700.328),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'dones': np.ndarray((64,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'new_obs': np.ndarray((64, 2), dtype=float32, min=-2.021, max=1.696, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'obs': np.ndarray((64, 2), dtype=float32, min=-1.987, max=2.672, mean=0.085),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'rewards': np.ndarray((64,), dtype=float32, min=0.191, max=1.278, mean=0.997),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'weights': np.ndarray((64,), dtype=float64, min=0.98, max=0.997, mean=0.993)},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                    'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                       'agent_1': { 'data': { 'actions': np.ndarray((64,), dtype=int64, min=0.0, max=4.0, mean=3.219),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'batch_indexes': np.ndarray((64,), dtype=int64, min=77.0, max=5064.0, mean=2519.172),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'dones': np.ndarray((64,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'new_obs': np.ndarray((64, 2), dtype=float32, min=-2.763, max=2.198, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'obs': np.ndarray((64, 2), dtype=float32, min=-2.148, max=2.514, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'rewards': np.ndarray((64,), dtype=float32, min=0.079, max=1.262, mean=0.535),\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                              'weights': np.ndarray((64,), dtype=float64, min=0.975, max=0.997, mean=0.992)},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                    'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/action_value/hidden_0/kernel:0' shape=(24, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/action_value/hidden_0/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/action_value/dense/kernel:0' shape=(16, 50) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/action_value/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/state_value/dense/kernel:0' shape=(24, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/state_value/dense/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/state_value/dense_1/kernel:0' shape=(16, 10) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/state_value/dense_1/bias:0' shape=(10,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/fc1/kernel:0' shape=(2, 24) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,547\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/fc1/bias:0' shape=(24,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,548\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/fc_out/kernel:0' shape=(24, 24) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,548\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'agent_0/q_func/fc_out/bias:0' shape=(24,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:22,548\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m 2019-08-03 12:31:23,072\tINFO rollout_worker.py:588 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m { 'agent_0': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                   'mean_td_error': 2.3602953,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                   'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                'td_error': np.ndarray((64,), dtype=float32, min=2.315, max=2.52, mean=2.36)},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m   'agent_1': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                   'mean_td_error': 2.3502269,\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                                   'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m                'td_error': np.ndarray((64,), dtype=float32, min=2.289, max=2.567, mean=2.35)}}\n",
      "\u001b[2m\u001b[36m(pid=4909)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 12:31:41.077734 139735645222720 deprecation_wrapper.py:119] From /home/lorenzo/anaconda3/envs/tf-rllib-2/lib/python3.6/site-packages/ray/tune/logger.py:117: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 1.8191215035274917\n",
      "    delta0_mean: 1.7617924532577973\n",
      "    delta0_min: 1.6905625984705683\n",
      "    delta1_max: -0.32425386503906256\n",
      "    delta1_mean: -0.5414074408759946\n",
      "    delta1_min: -0.7103533419550015\n",
      "    price0_max: 1.6904187543732507\n",
      "    price0_mean: 1.6458039342856605\n",
      "    price0_min: 1.6082153076269492\n",
      "    price1_max: 1.9980574176579369\n",
      "    price1_mean: 1.9900223016262248\n",
      "    price1_min: 1.976213030412835\n",
      "  date: 2019-08-03_12-31-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 64.33840156224525\n",
      "  episode_reward_mean: 59.412821892517314\n",
      "  episode_reward_min: 55.34838779358367\n",
      "  episodes_this_iter: 200\n",
      "  episodes_total: 200\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 2.1844780445098877\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 2.247044563293457\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 24521\n",
      "      size_mean: 0.36\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.5919459434779497\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 42496\n",
      "    num_steps_trained: 91392\n",
      "    num_target_updates: 1\n",
      "    num_weight_syncs: 83\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 22.533\n",
      "      policy_agent_0:\n",
      "        added_count: 9088\n",
      "        est_size_bytes: 11841896\n",
      "        num_entries: 9088\n",
      "        sampled_count: 22272\n",
      "      policy_agent_1:\n",
      "        added_count: 9088\n",
      "        est_size_bytes: 11841896\n",
      "        num_entries: 9088\n",
      "        sampled_count: 22272\n",
      "      replay_time_ms: 19.582\n",
      "      update_priorities_time_ms: 17.87\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 3558.771\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 42.693667027175486\n",
      "    agent_1: 16.719154865341828\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.0922499434901076\n",
      "    mean_inference_ms: 1.7447137851031005\n",
      "    mean_processing_ms: 4.460221039389167\n",
      "  time_since_restore: 30.23485565185547\n",
      "  time_this_iter_s: 30.23485565185547\n",
      "  time_total_s: 30.23485565185547\n",
      "  timestamp: 1564828301\n",
      "  timesteps_since_restore: 42496\n",
      "  timesteps_this_iter: 42496\n",
      "  timesteps_total: 42496\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.2/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 30 s, 1 iter, 42496 ts, 59.4 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 1.614757787809301\n",
      "    delta0_mean: 1.5427643865194942\n",
      "    delta0_min: 1.3531000858795403\n",
      "    delta1_max: -0.5294739434651565\n",
      "    delta1_mean: -0.761228720401362\n",
      "    delta1_min: -0.9134217764425906\n",
      "    price0_max: 1.6000119007616487\n",
      "    price0_mean: 1.5728730750610145\n",
      "    price0_min: 1.5401761239035088\n",
      "    price1_max: 1.9937161767826825\n",
      "    price1_mean: 1.9764404987708686\n",
      "    price1_min: 1.9074901846349746\n",
      "  date: 2019-08-03_12-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 60.58253234186765\n",
      "  episode_reward_mean: 50.02304748560547\n",
      "  episode_reward_min: 41.512441534781246\n",
      "  episodes_this_iter: 224\n",
      "  episodes_total: 424\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.9347517490386963\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 2.059921979904175\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 27560\n",
      "      size_mean: 0.46\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.6390618123468182\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 87552\n",
      "    num_steps_trained: 285824\n",
      "    num_target_updates: 5\n",
      "    num_weight_syncs: 171\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 15.12\n",
      "      policy_agent_0:\n",
      "        added_count: 20736\n",
      "        est_size_bytes: 27019648\n",
      "        num_entries: 20736\n",
      "        sampled_count: 69952\n",
      "      policy_agent_1:\n",
      "        added_count: 20736\n",
      "        est_size_bytes: 27019648\n",
      "        num_entries: 20736\n",
      "        sampled_count: 69952\n",
      "      replay_time_ms: 14.79\n",
      "      update_priorities_time_ms: 13.187\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 3859.548\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 37.99906256650926\n",
      "    agent_1: 12.023984919096247\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.1510084899816702\n",
      "    mean_inference_ms: 1.834757022482976\n",
      "    mean_processing_ms: 4.711747682462573\n",
      "  time_since_restore: 60.386303424835205\n",
      "  time_this_iter_s: 30.151447772979736\n",
      "  time_total_s: 60.386303424835205\n",
      "  timestamp: 1564828331\n",
      "  timesteps_since_restore: 87552\n",
      "  timesteps_this_iter: 45056\n",
      "  timesteps_total: 87552\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 60 s, 2 iter, 87552 ts, 50 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.9980849405914622\n",
      "    delta0_mean: 0.8919072660495788\n",
      "    delta0_min: 0.5803766260481317\n",
      "    delta1_max: -0.5849514068160735\n",
      "    delta1_mean: -0.7812672311907023\n",
      "    delta1_min: -0.8959296419742274\n",
      "    price0_max: 1.5616036538096443\n",
      "    price0_mean: 1.537588880659885\n",
      "    price0_min: 1.5128022263795722\n",
      "    price1_max: 1.815808521073282\n",
      "    price1_mean: 1.7855976096528525\n",
      "    price1_min: 1.6768814123607771\n",
      "  date: 2019-08-03_12-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 51.073684849836475\n",
      "  episode_reward_mean: 31.458585670845064\n",
      "  episode_reward_min: 25.51715942557726\n",
      "  episodes_this_iter: 208\n",
      "  episodes_total: 632\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.7115135192871094\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.850162386894226\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 30283\n",
      "      size_mean: 0.26\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.48207883172775806\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 127232\n",
      "    num_steps_trained: 460160\n",
      "    num_target_updates: 9\n",
      "    num_weight_syncs: 248\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 21.611\n",
      "      policy_agent_0:\n",
      "        added_count: 30592\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 111936\n",
      "      policy_agent_1:\n",
      "        added_count: 30592\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 111936\n",
      "      replay_time_ms: 24.786\n",
      "      update_priorities_time_ms: 16.594\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 4542.081\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 17.907486516424473\n",
      "    agent_1: 13.551099154420596\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.174464488839158\n",
      "    mean_inference_ms: 1.8904640162597628\n",
      "    mean_processing_ms: 4.961229999052455\n",
      "  time_since_restore: 90.63615107536316\n",
      "  time_this_iter_s: 30.249847650527954\n",
      "  time_total_s: 90.63615107536316\n",
      "  timestamp: 1564828361\n",
      "  timesteps_since_restore: 127232\n",
      "  timesteps_this_iter: 39680\n",
      "  timesteps_total: 127232\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 90 s, 3 iter, 127232 ts, 31.5 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.49058185529550663\n",
      "    delta0_mean: 0.4099090618811463\n",
      "    delta0_min: 0.17335372891185458\n",
      "    delta1_max: -0.7126012245746741\n",
      "    delta1_mean: -0.8617263602669347\n",
      "    delta1_min: -0.9491393786183794\n",
      "    price0_max: 1.4800997575353332\n",
      "    price0_mean: 1.4618696989021487\n",
      "    price0_min: 1.4428582775995096\n",
      "    price1_max: 1.6679032139337564\n",
      "    price1_mean: 1.644959088782028\n",
      "    price1_min: 1.5623706179093357\n",
      "  date: 2019-08-03_12-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 24.12800318306712\n",
      "  episode_reward_mean: 20.507237149253193\n",
      "  episode_reward_min: 19.617587393616972\n",
      "  episodes_this_iter: 200\n",
      "  episodes_total: 832\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.696120023727417\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.6970953941345215\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 33103\n",
      "      size_mean: 0.24\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.47159304490206383\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 168320\n",
      "    num_steps_trained: 640704\n",
      "    num_target_updates: 12\n",
      "    num_weight_syncs: 328\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 19.439\n",
      "      policy_agent_0:\n",
      "        added_count: 41088\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 156288\n",
      "      policy_agent_1:\n",
      "        added_count: 41088\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 156288\n",
      "      replay_time_ms: 19.054\n",
      "      update_priorities_time_ms: 10.956\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 0.0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 10.268501252762666\n",
      "    agent_1: 10.238735896490512\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.1872109815349143\n",
      "    mean_inference_ms: 1.9196972025227768\n",
      "    mean_processing_ms: 5.12643574509971\n",
      "  time_since_restore: 120.84671664237976\n",
      "  time_this_iter_s: 30.2105655670166\n",
      "  time_total_s: 120.84671664237976\n",
      "  timestamp: 1564828391\n",
      "  timesteps_since_restore: 168320\n",
      "  timesteps_this_iter: 41088\n",
      "  timesteps_total: 168320\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 120 s, 4 iter, 168320 ts, 20.5 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.44290609215420756\n",
      "    delta0_mean: 0.3776964452699449\n",
      "    delta0_min: 0.18847687513763156\n",
      "    delta1_max: -0.6888487807182837\n",
      "    delta1_mean: -0.8001291618758264\n",
      "    delta1_min: -0.8664918625164457\n",
      "    price0_max: 1.4629427292048316\n",
      "    price0_mean: 1.449561750713209\n",
      "    price0_min: 1.4347454840040836\n",
      "    price1_max: 1.6538025931942217\n",
      "    price1_mean: 1.634092351460528\n",
      "    price1_min: 1.5696270949021067\n",
      "  date: 2019-08-03_12-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 50.98407130642039\n",
      "  episode_reward_mean: 42.15354753290197\n",
      "  episode_reward_min: 24.21395664929382\n",
      "  episodes_this_iter: 216\n",
      "  episodes_total: 1048\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.3524906635284424\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.5314922332763672\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 36126\n",
      "      size_mean: 0.2\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 1.0\n",
      "      size_std: 0.4\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 211456\n",
      "    num_steps_trained: 834112\n",
      "    num_target_updates: 16\n",
      "    num_weight_syncs: 413\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 16.201\n",
      "      policy_agent_0:\n",
      "        added_count: 51584\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 204352\n",
      "      policy_agent_1:\n",
      "        added_count: 51584\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 204352\n",
      "      replay_time_ms: 18.415\n",
      "      update_priorities_time_ms: 11.057\n",
      "    sample_throughput: 3647.716\n",
      "    train_throughput: 9119.291\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 25.679795347662868\n",
      "    agent_1: 16.473752185239096\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.1833939073403543\n",
      "    mean_inference_ms: 1.8982785396826796\n",
      "    mean_processing_ms: 5.212840495214329\n",
      "  time_since_restore: 151.09626865386963\n",
      "  time_this_iter_s: 30.249552011489868\n",
      "  time_total_s: 151.09626865386963\n",
      "  timestamp: 1564828422\n",
      "  timesteps_since_restore: 211456\n",
      "  timesteps_this_iter: 43136\n",
      "  timesteps_total: 211456\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 151 s, 5 iter, 211456 ts, 42.2 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.4776428164817491\n",
      "    delta0_mean: 0.4235474006155738\n",
      "    delta0_min: 0.26380000420687233\n",
      "    delta1_max: -0.6041671418768514\n",
      "    delta1_mean: -0.697097803530014\n",
      "    delta1_min: -0.7538665555524647\n",
      "    price0_max: 1.4698885613302033\n",
      "    price0_mean: 1.4588699492420358\n",
      "    price0_min: 1.4462593293298507\n",
      "    price1_max: 1.6568500466716878\n",
      "    price1_mean: 1.6398412805819738\n",
      "    price1_min: 1.5855466496538684\n",
      "  date: 2019-08-03_12-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 56.89733636869467\n",
      "  episode_reward_mean: 51.34365996415481\n",
      "  episode_reward_min: 43.27190316263954\n",
      "  episodes_this_iter: 200\n",
      "  episodes_total: 1248\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.9988733530044556\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.4396300315856934\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 39153\n",
      "      size_mean: 0.2\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.447213595499958\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 251648\n",
      "    num_steps_trained: 1027776\n",
      "    num_target_updates: 20\n",
      "    num_weight_syncs: 491\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 18.686\n",
      "      policy_agent_0:\n",
      "        added_count: 61568\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 250752\n",
      "      policy_agent_1:\n",
      "        added_count: 61568\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 250752\n",
      "      replay_time_ms: 19.547\n",
      "      update_priorities_time_ms: 12.819\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 2838.034\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 30.298182242150656\n",
      "    agent_1: 21.04547772200416\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.1881231811292852\n",
      "    mean_inference_ms: 1.8912375244279362\n",
      "    mean_processing_ms: 5.385656671914262\n",
      "  time_since_restore: 181.32961344718933\n",
      "  time_this_iter_s: 30.233344793319702\n",
      "  time_total_s: 181.32961344718933\n",
      "  timestamp: 1564828452\n",
      "  timesteps_since_restore: 251648\n",
      "  timesteps_this_iter: 40192\n",
      "  timesteps_total: 251648\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 181 s, 6 iter, 251648 ts, 51.3 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.52324451916258\n",
      "    delta0_mean: 0.47627290445594134\n",
      "    delta0_min: 0.337611214974328\n",
      "    delta1_max: -0.49026175235612174\n",
      "    delta1_mean: -0.570946216215864\n",
      "    delta1_min: -0.6204212683278157\n",
      "    price0_max: 1.4892091498388977\n",
      "    price0_mean: 1.4796383111094522\n",
      "    price0_min: 1.4686141026019388\n",
      "    price1_max: 1.6634446415303317\n",
      "    price1_mean: 1.6487147130606459\n",
      "    price1_min: 1.6016315065829676\n",
      "  date: 2019-08-03_12-34-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 61.16189458182466\n",
      "  episode_reward_mean: 57.67107213353976\n",
      "  episode_reward_min: 54.394532033934986\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.6498583555221558\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.3861396312713623\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 42021\n",
      "      size_mean: 0.38\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      size_std: 0.6896375859826668\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 289664\n",
      "    num_steps_trained: 1211392\n",
      "    num_target_updates: 24\n",
      "    num_weight_syncs: 565\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 20.576\n",
      "      policy_agent_0:\n",
      "        added_count: 71296\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 296256\n",
      "      policy_agent_1:\n",
      "        added_count: 71296\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 296256\n",
      "      replay_time_ms: 16.545\n",
      "      update_priorities_time_ms: 11.805\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 4722.651\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 32.04766674404037\n",
      "    agent_1: 25.623405389499393\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.189830303324234\n",
      "    mean_inference_ms: 1.9003233227203264\n",
      "    mean_processing_ms: 5.532832560205574\n",
      "  time_since_restore: 211.49373531341553\n",
      "  time_this_iter_s: 30.164121866226196\n",
      "  time_total_s: 211.49373531341553\n",
      "  timestamp: 1564828482\n",
      "  timesteps_since_restore: 289664\n",
      "  timesteps_this_iter: 38016\n",
      "  timesteps_total: 289664\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 211 s, 7 iter, 289664 ts, 57.7 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.5218237032945073\n",
      "    delta0_mean: 0.4805359050028659\n",
      "    delta0_min: 0.3584450331578167\n",
      "    delta1_max: -0.3499109714703114\n",
      "    delta1_mean: -0.4205807282289095\n",
      "    delta1_min: -0.4643957557013935\n",
      "    price0_max: 1.5126979800528022\n",
      "    price0_mean: 1.5043749175916725\n",
      "    price0_min: 1.4946107811814058\n",
      "    price1_max: 1.662897493948466\n",
      "    price1_mean: 1.6499444172502258\n",
      "    price1_min: 1.6084972798582995\n",
      "  date: 2019-08-03_12-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 60.77070630336997\n",
      "  episode_reward_mean: 58.85787512066652\n",
      "  episode_reward_min: 56.56004732215712\n",
      "  episodes_this_iter: 200\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5789761543273926\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.2916091680526733\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 44964\n",
      "      size_mean: 0.38\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.1000000000000014\n",
      "      - 2.0\n",
      "      size_std: 0.66\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 329472\n",
      "    num_steps_trained: 1399808\n",
      "    num_target_updates: 27\n",
      "    num_weight_syncs: 643\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 15.348\n",
      "      policy_agent_0:\n",
      "        added_count: 80768\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 343168\n",
      "      policy_agent_1:\n",
      "        added_count: 80768\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 343168\n",
      "      replay_time_ms: 19.699\n",
      "      update_priorities_time_ms: 12.156\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 7379.872\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 28.577217946108398\n",
      "    agent_1: 30.28065717455813\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.183366559497031\n",
      "    mean_inference_ms: 1.88927754046269\n",
      "    mean_processing_ms: 5.65197856980563\n",
      "  time_since_restore: 241.85455179214478\n",
      "  time_this_iter_s: 30.360816478729248\n",
      "  time_total_s: 241.85455179214478\n",
      "  timestamp: 1564828513\n",
      "  timesteps_since_restore: 329472\n",
      "  timesteps_this_iter: 39808\n",
      "  timesteps_total: 329472\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 241 s, 8 iter, 329472 ts, 58.9 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.5032500650041796\n",
      "    delta0_mean: 0.46550391782309003\n",
      "    delta0_min: 0.3535770553069184\n",
      "    delta1_max: -0.2730986618587655\n",
      "    delta1_mean: -0.3377949863648264\n",
      "    delta1_min: -0.3778449398281895\n",
      "    price0_max: 1.523306934190996\n",
      "    price0_mean: 1.5156982661645266\n",
      "    price0_min: 1.5067933891762644\n",
      "    price1_max: 1.6571643966089684\n",
      "    price1_mean: 1.6452615466385427\n",
      "    price1_min: 1.6072842032721752\n",
      "  date: 2019-08-03_12-35-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 56.80061279783538\n",
      "  episode_reward_mean: 55.314162406003526\n",
      "  episode_reward_min: 53.533291509621215\n",
      "  episodes_this_iter: 152\n",
      "  episodes_total: 1792\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.563331663608551\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 1.1262142658233643\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 47555\n",
      "      size_mean: 0.26\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 3.0\n",
      "      size_std: 0.5936328831862332\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 361472\n",
      "    num_steps_trained: 1565632\n",
      "    num_target_updates: 31\n",
      "    num_weight_syncs: 705\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 35.324\n",
      "      policy_agent_0:\n",
      "        added_count: 89216\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 384896\n",
      "      policy_agent_1:\n",
      "        added_count: 89216\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 384896\n",
      "      replay_time_ms: 32.577\n",
      "      update_priorities_time_ms: 21.311\n",
      "    sample_throughput: 8356.358\n",
      "    train_throughput: 4178.179\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 26.23471049715972\n",
      "    agent_1: 29.079451908843804\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.2122145002625337\n",
      "    mean_inference_ms: 1.9266905085217467\n",
      "    mean_processing_ms: 5.891639979889099\n",
      "  time_since_restore: 272.14696884155273\n",
      "  time_this_iter_s: 30.29241704940796\n",
      "  time_total_s: 272.14696884155273\n",
      "  timestamp: 1564828543\n",
      "  timesteps_since_restore: 361472\n",
      "  timesteps_this_iter: 32000\n",
      "  timesteps_total: 361472\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 272 s, 9 iter, 361472 ts, 55.3 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.4862375807731779\n",
      "    delta0_mean: 0.4512415278780369\n",
      "    delta0_min: 0.34903199713767613\n",
      "    delta1_max: -0.19681929119916164\n",
      "    delta1_mean: -0.25632963802508885\n",
      "    delta1_min: -0.292957663164271\n",
      "    price0_max: 1.534140467803646\n",
      "    price0_mean: 1.5270572040707187\n",
      "    price0_min: 1.5188810684143936\n",
      "    price1_max: 1.6518103938864275\n",
      "    price1_mean: 1.6409822048963967\n",
      "    price1_min: 1.6064044652656395\n",
      "  date: 2019-08-03_12-36-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 58.96364547807282\n",
      "  episode_reward_mean: 55.5454452811915\n",
      "  episode_reward_min: 53.54631507879828\n",
      "  episodes_this_iter: 176\n",
      "  episodes_total: 1968\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5688899755477905\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.9802749156951904\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 50428\n",
      "      size_mean: 0.24\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.0\n",
      "      - 2.0\n",
      "      size_std: 0.5122499389946279\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 396032\n",
      "    num_steps_trained: 1749504\n",
      "    num_target_updates: 34\n",
      "    num_weight_syncs: 772\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 15.793\n",
      "      policy_agent_0:\n",
      "        added_count: 98176\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 430080\n",
      "      policy_agent_1:\n",
      "        added_count: 98176\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 430080\n",
      "      replay_time_ms: 15.473\n",
      "      update_priorities_time_ms: 12.375\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 6107.469\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 26.26647270641488\n",
      "    agent_1: 29.278972574776628\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.2139886111225309\n",
      "    mean_inference_ms: 1.9287330108232903\n",
      "    mean_processing_ms: 6.066908428122363\n",
      "  time_since_restore: 302.39163851737976\n",
      "  time_this_iter_s: 30.244669675827026\n",
      "  time_total_s: 302.39163851737976\n",
      "  timestamp: 1564828573\n",
      "  timesteps_since_restore: 396032\n",
      "  timesteps_this_iter: 34560\n",
      "  timesteps_total: 396032\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 302 s, 10 iter, 396032 ts, 55.5 rew\n",
      "\n",
      "Result for APEX_MultiAgentFirmsPricingContinuous_0:\n",
      "  custom_metrics:\n",
      "    delta0_max: 0.47721014365992553\n",
      "    delta0_mean: 0.4451746190183389\n",
      "    delta0_min: 0.35052584323413416\n",
      "    delta1_max: -0.14341705578345879\n",
      "    delta1_mean: -0.19825015184305173\n",
      "    delta1_min: -0.23261552032336347\n",
      "    price0_max: 1.5414880652395586\n",
      "    price0_mean: 1.5350078759007533\n",
      "    price0_min: 1.5272898208126575\n",
      "    price1_max: 1.6486288770276494\n",
      "    price1_mean: 1.6385795409991046\n",
      "    price1_min: 1.6065384862763619\n",
      "  date: 2019-08-03_12-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 59.561304515729496\n",
      "  episode_reward_mean: 55.62032560462237\n",
      "  episode_reward_min: 50.75450494164535\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2128\n",
      "  experiment_id: 95996b9e37e94167993e131b44c649e9\n",
      "  hostname: lorenzo-VirtualBox\n",
      "  info:\n",
      "    learner:\n",
      "      agent_0:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.6060782670974731\n",
      "        model: {}\n",
      "      agent_1:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.8581339120864868\n",
      "        model: {}\n",
      "    learner_queue:\n",
      "      size_count: 53058\n",
      "      size_mean: 0.52\n",
      "      size_quantiles:\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 0.0\n",
      "      - 1.1000000000000014\n",
      "      - 3.0\n",
      "      size_std: 0.7807688518377254\n",
      "    max_exploration: 0.4\n",
      "    min_exploration: 0.0\n",
      "    num_samples_dropped: 695\n",
      "    num_steps_sampled: 428160\n",
      "    num_steps_trained: 1917760\n",
      "    num_target_updates: 38\n",
      "    num_weight_syncs: 836\n",
      "    replay_shard_0:\n",
      "      add_batch_time_ms: 23.328\n",
      "      policy_agent_0:\n",
      "        added_count: 105472\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 472576\n",
      "      policy_agent_1:\n",
      "        added_count: 105472\n",
      "        est_size_bytes: 32575812\n",
      "        num_entries: 25000\n",
      "        sampled_count: 472576\n",
      "      replay_time_ms: 30.015\n",
      "      update_priorities_time_ms: 22.083\n",
      "    sample_throughput: 0.0\n",
      "    train_throughput: 0.0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.0.2.15\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  pid: 4909\n",
      "  policy_reward_mean:\n",
      "    agent_0: 26.990810293083644\n",
      "    agent_1: 28.629515311538732\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.2215269959312354\n",
      "    mean_inference_ms: 1.9368444285264146\n",
      "    mean_processing_ms: 6.261705033002899\n",
      "  time_since_restore: 332.6013514995575\n",
      "  time_this_iter_s: 30.209712982177734\n",
      "  time_total_s: 332.6013514995575\n",
      "  timestamp: 1564828604\n",
      "  timesteps_since_restore: 428160\n",
      "  timesteps_this_iter: 32128\n",
      "  timesteps_total: 428160\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 5/6 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.9/10.2 GB\n",
      "Result logdir: /home/lorenzo/ray_results/20_cont_DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - APEX_MultiAgentFirmsPricingContinuous_0:\tRUNNING, [5 CPUs, 0 GPUs], [pid=4909], 332 s, 11 iter, 428160 ts, 55.6 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "trial = tune.run(\n",
    "        run_or_experiment= 'APEX',\n",
    "        name='20_cont_DQN',\n",
    "        stop={\"timesteps_total\":10**8},\n",
    "        checkpoint_freq=50,\n",
    "        #resume=False,\n",
    "        #num_samples = 2,\n",
    "        config={\n",
    "            \"env\": MultiAgentFirmsPricingContinuous,\n",
    "            \"env_config\": ENV_CONFIG,\n",
    "            \"horizon\": 100,\n",
    "            \"soft_horizon\": True,\n",
    "            \"double_q\": True,\n",
    "            \"dueling\": True,\n",
    "            \"hiddens\": [16],\n",
    "            \"n_step\": 3,\n",
    "            \"num_atoms\": 10,\n",
    "            #\"noisy\": True,\n",
    "            #\"sigma0\": 0.5,\n",
    "            \"gamma\": 0.975,\n",
    "            \"prioritized_replay\": True,\n",
    "            \"prioritized_replay_alpha\": 0.5,\n",
    "            \"beta_annealing_fraction\": 0.2,\n",
    "            \"final_prioritized_replay_beta\": 1.0,\n",
    "            \"learning_starts\": 20000,\n",
    "            \"lr\":0.0005,\n",
    "            \"adam_epsilon\": 0.00015,\n",
    "            \"schedule_max_timesteps\": 10**7,\n",
    "            \"exploration_final_eps\":0.02,\n",
    "            \"exploration_fraction\":0.02,\n",
    "            \"buffer_size\": 10**5,\n",
    "            \"target_network_update_freq\": 50000,\n",
    "            \"sample_batch_size\":16,\n",
    "            \"train_batch_size\":64,\n",
    "            \n",
    "            \"observation_filter\": \"MeanStdFilter\",\n",
    "            \"num_workers\": 2,\n",
    "            \"num_envs_per_worker\": 8,\n",
    "            \"num_cpus_per_worker\": 2,\n",
    "            #\"num_cpus_for_driver\": 1,\n",
    "            \"num_gpus\":0,\n",
    "            \"multiagent\": {\n",
    "                    \"policy_graphs\": policy_graphs,\n",
    "                    \"policy_mapping_fn\": tune.function(policy_mapping_fn)\n",
    "            },\n",
    "            \"model\": {\n",
    "                    \"fcnet_activation\": \"tanh\",\n",
    "                    \"fcnet_hiddens\":[24, 24],\n",
    "                    },\n",
    "            \"callbacks\": {\n",
    "                    \"on_episode_start\": tune.function(on_episode_start),\n",
    "                    \"on_episode_step\": tune.function(on_episode_step),\n",
    "                    \"on_episode_end\": tune.function(on_episode_end),\n",
    "                    },\n",
    "            },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
